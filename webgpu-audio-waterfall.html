<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>WebGPU Audio Spectrum Waterfall</title>
    <style>
        body {
            font-family: system-ui, -apple-system, sans-serif;
            max-width: 1200px;
            margin: 40px auto;
            padding: 0 20px;
            background: #1a1a1a;
            color: #fff;
        }
        h1 {
            color: #fff;
        }
        .controls {
            background: #2a2a2a;
            padding: 20px;
            border-radius: 8px;
            margin: 20px 0;
        }
        button {
            padding: 12px 24px;
            font-size: 16px;
            background: #3b82f6;
            color: white;
            border: none;
            border-radius: 6px;
            cursor: pointer;
            font-weight: 500;
            margin-right: 10px;
        }
        button:hover:not(:disabled) {
            background: #2563eb;
        }
        button:disabled {
            background: #9ca3af;
            cursor: not-allowed;
        }
        button.stop {
            background: #ef4444;
        }
        button.stop:hover:not(:disabled) {
            background: #dc2626;
        }
        .canvas-container {
            display: flex;
            gap: 20px;
            justify-content: center;
            margin: 20px 0;
        }
        .canvas-wrapper {
            flex: 1;
            max-width: 512px;
        }
        .canvas-wrapper h3 {
            text-align: center;
            margin: 0 0 10px 0;
            color: #9ca3af;
            font-size: 14px;
        }
        canvas {
            display: block;
            width: 100%;
            height: 512px;
            background: #000;
            border-radius: 8px;
        }
        .status {
            padding: 10px;
            background: #374151;
            border-radius: 4px;
            margin-top: 10px;
        }
        .error {
            color: #ef4444;
            background: #fee;
            padding: 10px;
            border-radius: 4px;
            margin: 10px 0;
        }
        .info {
            background: #2a2a2a;
            padding: 15px;
            border-radius: 8px;
            margin: 20px 0;
            font-size: 14px;
        }
        .button-group {
            margin-bottom: 15px;
            padding-bottom: 15px;
            border-bottom: 1px solid #4b5563;
        }
        .button-group:last-of-type {
            border-bottom: none;
            margin-bottom: 0;
            padding-bottom: 0;
        }
        .button-group label {
            display: block;
            color: #9ca3af;
            font-size: 12px;
            font-weight: 600;
            margin-bottom: 8px;
            text-transform: uppercase;
            letter-spacing: 0.5px;
        }
        input[type="file"] {
            display: block;
            margin: 10px 0;
            color: #fff;
            font-size: 14px;
        }
        input[type="file"]::file-selector-button {
            padding: 8px 16px;
            background: #3b82f6;
            color: white;
            border: none;
            border-radius: 4px;
            cursor: pointer;
            font-weight: 500;
            margin-right: 10px;
        }
        input[type="file"]::file-selector-button:hover {
            background: #2563eb;
        }
        button.test {
            background: #8b5cf6;
        }
        button.test:hover:not(:disabled) {
            background: #7c3aed;
        }
    </style>
</head>
<body>
    <h1>WebGPU Audio Spectrum Waterfall</h1>

    <div class="info">
        <strong>Dual Waterfall - Input/Output Comparison</strong><br>
        <strong>Option 1:</strong> Record from microphone → Stop → Process → Compare<br>
        <strong>Option 2:</strong> Upload audio file → Process → Compare<br>
        <strong>Option 3:</strong> Select noise type & SNR level → Load test → Process → Compare<br>
        <strong>Frequency range: 0 - 4,000 Hz</strong> (optimized for human speech) | X-axis: Frequency bins | Y-axis: Time scrolling down
    </div>

    <div class="controls">
        <div class="button-group">
            <label>Record from Microphone</label>
            <button id="startBtn" onclick="start()">Start Recording</button>
            <button id="stopBtn" onclick="stop()" class="stop" disabled>Stop</button>
        </div>

        <div class="button-group">
            <label>Load Audio File</label>
            <input type="file" id="fileInput" accept="audio/*" onchange="loadAudioFile(event)" />
        </div>

        <div class="button-group">
            <label>Load Test Dataset</label>
            <div style="display: flex; gap: 10px; align-items: center; margin-bottom: 10px;">
                <select id="noiseType" style="padding: 8px; border-radius: 4px; background: #374151; color: #fff; border: 1px solid #4b5563; flex: 1;">
                    <option value="pink">Pink Noise</option>
                    <option value="white">White Noise</option>
                    <option value="brown">Brown Noise</option>
                    <option value="cafe">Cafe Ambience</option>
                </select>
                <select id="snrLevel" style="padding: 8px; border-radius: 4px; background: #374151; color: #fff; border: 1px solid #4b5563; flex: 1;">
                    <option value="0">SNR: 0 dB (very noisy)</option>
                    <option value="5">SNR: 5 dB (noisy)</option>
                    <option value="10" selected>SNR: 10 dB (moderate)</option>
                    <option value="15">SNR: 15 dB (mild)</option>
                    <option value="20">SNR: 20 dB (clean)</option>
                </select>
            </div>
            <button id="testFileBtn" onclick="loadTestFile()" class="test">Load Selected Test</button>
        </div>

        <div class="button-group">
            <label>Noise Reduction Parameters</label>
            <div style="display: grid; grid-template-columns: 150px 1fr 80px; gap: 10px; margin-bottom: 10px; align-items: center; font-size: 14px;">
                <label for="noiseGain" style="margin: 0;" title="Gain applied when signal ≤ noise (0 = full cut, 1 = no reduction)">Noise Gain:</label>
                <input type="range" id="noiseGain" min="0" max="1" step="0.05" value="0.2" style="width: 100%;" oninput="updateParamDisplay()" title="Lower = more aggressive noise removal">
                <span id="noiseGainValue" style="color: #9ca3af;">0.20</span>

                <label for="signalGain" style="margin: 0;" title="Gain applied when signal > noise (typically 1.0)">Signal Gain:</label>
                <input type="range" id="signalGain" min="0" max="2" step="0.1" value="1.0" style="width: 100%;" oninput="updateParamDisplay()" title="Gain for speech (1.0 = unchanged)">
                <span id="signalGainValue" style="color: #9ca3af;">1.00</span>

                <label for="alphaDecay" style="margin: 0;" title="Learning rate for noise profile adaptation (lower = slower adaptation)">Alpha Decay:</label>
                <input type="range" id="alphaDecay" min="0.001" max="0.1" step="0.001" value="0.01" style="width: 100%;" oninput="updateParamDisplay()" title="How fast noise estimate adapts">
                <span id="alphaDecayValue" style="color: #9ca3af;">0.010</span>
            </div>
            <button onclick="applyParameters()" style="background: #059669; margin-bottom: 10px;" title="Recreate GPU pipelines with new parameter values">Apply Parameters</button>
            <button onclick="resetParameters()" style="background: #6b7280;" title="Reset all parameters to default values">Reset to Defaults</button>
        </div>

        <div class="button-group">
            <label>Process & Playback</label>
            <button id="processBtn" onclick="processRecording()" disabled>Process Audio</button>
            <button id="playInputBtn" onclick="playInput()" disabled>Play Input</button>
            <button id="playOutputBtn" onclick="playOutput()" disabled>Play Output</button>
        </div>

        <div id="status" class="status">Record from microphone or load an audio file to begin</div>
        <div id="debug" style="margin-top: 10px; font-family: monospace; font-size: 12px;"></div>
    </div>

    <div class="canvas-container">
        <div class="canvas-wrapper">
            <h3>INPUT</h3>
            <canvas id="inputCanvas" width="512" height="512"></canvas>
        </div>
        <div class="canvas-wrapper">
            <h3>OUTPUT (Processed)</h3>
            <canvas id="outputCanvas" width="512" height="512"></canvas>
        </div>
    </div>

    <script>
        const FFT_SIZE = 1024;
        const WATERFALL_HEIGHT = 512;
        const SAMPLE_RATE = 44100;

        // Focus on human speech range: 0-8000 Hz instead of full 0-22050 Hz
        const MAX_DISPLAY_FREQ = 4000; // Hz
        const FREQ_PER_BIN = SAMPLE_RATE / FFT_SIZE; // ~86 Hz per bin
        const WATERFALL_WIDTH = Math.floor(MAX_DISPLAY_FREQ / FREQ_PER_BIN); // ~186 bins for 0-8kHz

        let NOISE_GAIN = 0.2;
        let SIGNAL_GAIN = 1.0;
        let ALPHA_DECAY = 0.01;
        let NOISE_SUPRESSION_FACTOR = 3;

        let device = null;
        let inputContext = null;
        let outputContext = null;
        let renderPipeline = null;
        let fftPipeline = null;
        let ifftPipeline = null;
        let scrollPipeline = null;
        let noiseReductionPipeline = null;
        let updateNoisePipeline = null;

        let audioContext = null;
        let analyserNode = null;
        let micStream = null;
        let audioDataArray = null;
        let noiseProfile = null;

        let inputWaterfallTexture = null;
        let outputWaterfallTexture = null;
        let audioBuffer = null;
        let fftBuffer = null;
        let processedFFTBuffer = null;
        let processedAudioBuffer = null;
        let inputParamsBuffer = null;
        let outputParamsBuffer = null;

        let animationId = null;
        let isRunning = false;
        let isRecording = false;
        let inputCurrentRow = 0;
        let outputCurrentRow = 0;

        // Recording storage
        let recordedInputChunks = [];
        let recordedOutputChunks = [];
        let inputAudioBuffer = null;
        let outputAudioBuffer = null;

        async function initWebGPU() {
            if (device) return true;

            if (!navigator.gpu) {
                showError('WebGPU is not supported in this browser');
                return false;
            }

            try {
                const adapter = await navigator.gpu.requestAdapter();
                if (!adapter) {
                    showError('Failed to get GPU adapter');
                    return false;
                }

                device = await adapter.requestDevice();

                const inputCanvas = document.getElementById('inputCanvas');
                const outputCanvas = document.getElementById('outputCanvas');
                inputContext = inputCanvas.getContext('webgpu');
                outputContext = outputCanvas.getContext('webgpu');

                const presentationFormat = navigator.gpu.getPreferredCanvasFormat();
                inputContext.configure({
                    device,
                    format: presentationFormat,
                });
                outputContext.configure({
                    device,
                    format: presentationFormat,
                });

                await setupWebGPUResources();

                console.log('WebGPU initialized successfully');
                console.log('Waterfall texture:', WATERFALL_WIDTH, 'x', WATERFALL_HEIGHT);
                console.log('FFT size:', FFT_SIZE);

                return true;
            } catch (error) {
                showError('WebGPU initialization error: ' + error.message);
                return false;
            }
        }

        async function setupWebGPUResources() {
            // Create waterfall textures for input and output (width = freq bins, height = time)
            // Use rgba8unorm instead of r8unorm because it supports storage writes
            inputWaterfallTexture = device.createTexture({
                size: [WATERFALL_WIDTH, WATERFALL_HEIGHT],
                format: 'rgba8unorm',
                usage: GPUTextureUsage.TEXTURE_BINDING |
                       GPUTextureUsage.STORAGE_BINDING |
                       GPUTextureUsage.COPY_DST
            });

            outputWaterfallTexture = device.createTexture({
                size: [WATERFALL_WIDTH, WATERFALL_HEIGHT],
                format: 'rgba8unorm',
                usage: GPUTextureUsage.TEXTURE_BINDING |
                       GPUTextureUsage.STORAGE_BINDING |
                       GPUTextureUsage.COPY_DST
            });

            // Create buffers for audio data and FFT results
            audioBuffer = device.createBuffer({
                size: FFT_SIZE * 4, // Float32
                usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST
            });

            fftBuffer = device.createBuffer({
                size: FFT_SIZE * 2 * 4, // Complex numbers (real, imag)
                usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_SRC
            });

            processedFFTBuffer = device.createBuffer({
                size: FFT_SIZE * 2 * 4, // Complex numbers (real, imag)
                usage: GPUBufferUsage.STORAGE,
            });

            processedAudioBuffer = device.createBuffer({
                size: FFT_SIZE * 4, // Real floats
                usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_SRC
            });

            noiseProfile = device.createBuffer({
                size: WATERFALL_WIDTH * 4, // 4 bytes per f32
                usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST | GPUBufferUsage.COPY_SRC,
            });

            // Initialize noise profile with high values for minimum statistics
            const initNoiseProfile = new Float32Array(WATERFALL_WIDTH);
            initNoiseProfile.fill(1000.0); // Start high, will decrease to actual noise floor
            device.queue.writeBuffer(noiseProfile, 0, initNoiseProfile);
            console.log('Noise profile initialized with values:', initNoiseProfile.slice(0, 5), '... (total:', WATERFALL_WIDTH, 'bins)');

            inputParamsBuffer = device.createBuffer({
                size: 4, // u32 for current row
                usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST
            });

            outputParamsBuffer = device.createBuffer({
                size: 4, // u32 for current row
                usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST
            });

            // Initialize textures with black
            const blackData = new Uint8Array(WATERFALL_WIDTH * WATERFALL_HEIGHT * 4); // RGBA
            for (let i = 0; i < blackData.length; i += 4) {
                blackData[i + 3] = 255; // Alpha = 1
            }
            device.queue.writeTexture(
                { texture: inputWaterfallTexture },
                blackData,
                { bytesPerRow: WATERFALL_WIDTH * 4 },
                { width: WATERFALL_WIDTH, height: WATERFALL_HEIGHT }
            );
            device.queue.writeTexture(
                { texture: outputWaterfallTexture },
                blackData,
                { bytesPerRow: WATERFALL_WIDTH * 4 },
                { width: WATERFALL_WIDTH, height: WATERFALL_HEIGHT }
            );
            console.log('Textures initialized');

            // Create pipelines
            await createFFTPipeline();
            await createIFFTPipeline();
            await createNoiseReductionPipeline();
            await createUpdateNoisePipeline();
            await createScrollPipeline();
            await createRenderPipeline();
        }

        async function createFFTPipeline() {
            // Simple DFT for now (will be slow but functional)
            // TODO: Implement Cooley-Tukey FFT for better performance
            const fftShader = `
                @group(0) @binding(0) var<storage, read> input: array<f32>;
                @group(0) @binding(1) var<storage, read_write> output: array<vec2<f32>>;

                const FFT_SIZE: u32 = ${FFT_SIZE};
                const PI: f32 = 3.14159265359;

                @compute @workgroup_size(64)
                fn main(@builtin(global_invocation_id) global_id: vec3<u32>) {
                    let k = global_id.x;
                    if (k >= FFT_SIZE) {
                        return;
                    }

                    var sum_real: f32 = 0.0;
                    var sum_imag: f32 = 0.0;

                    for (var n: u32 = 0; n < FFT_SIZE; n = n + 1) {
                        let angle = -2.0 * PI * f32(k) * f32(n) / f32(FFT_SIZE);
                        sum_real = sum_real + input[n] * cos(angle);
                        sum_imag = sum_imag + input[n] * sin(angle);
                    }

                    output[k] = vec2<f32>(sum_real, sum_imag);
                }
            `;

            try {
                const module = device.createShaderModule({ code: fftShader });
                console.log('FFT shader module created');

                fftPipeline = device.createComputePipeline({
                    layout: 'auto',
                    compute: {
                        module,
                        entryPoint: 'main'
                    }
                });
                console.log('FFT pipeline created');
            } catch (error) {
                console.error('FFT pipeline creation error:', error);
                throw error;
            }
        }



        // { binding: 0, resource: { buffer: noiseProfile }},
        // { binding: 1, resource: { buffer: fftBuffer }},
        // { binding: 2, resource: { buffer: processedFFTBuffer }}
        async function createNoiseReductionPipeline() {
            const noiseReductionShader = `
            @group(0) @binding(0) var<storage, read_write> noiseProfile: array<f32>;
            @group(0) @binding(1) var<storage, read> fftBuffer: array<vec2<f32>>;
            @group(0) @binding(2) var<storage, read_write> processedFFTBuffer: array<vec2<f32>>;

            const FFT_SIZE: u32 = ${FFT_SIZE};
            const WATERFALL_WIDTH: u32 = ${WATERFALL_WIDTH};
            const NOISE_GAIN: f32 = ${NOISE_GAIN};
            const SIGNAL_GAIN: f32 = ${SIGNAL_GAIN};
            const NOISE_SUPRESSION_FACTOR: f32 = ${NOISE_SUPRESSION_FACTOR};

            @compute @workgroup_size(64)
            fn main(@builtin(global_invocation_id) global_id: vec3<u32>) {
                let k = global_id.x;
                if (k >= FFT_SIZE) {
                    return;
                }

                // Only apply noise reduction to speech frequency range
                if (k < WATERFALL_WIDTH) {
                    let noiseMagnitude = noiseProfile[k];
                    let signalMagnitude = length(fftBuffer[k]);

                    let cleanMagnitude = max(0.0, signalMagnitude - NOISE_SUPRESSION_FACTOR * noiseMagnitude);
                    let gain = cleanMagnitude / max(signalMagnitude, 0.001);
                    processedFFTBuffer[k] = fftBuffer[k] * gain;
                }
            }
            `;

            try {
                const module = device.createShaderModule({ code: noiseReductionShader });
                console.log('Noise Reduction shader module created');

                noiseReductionPipeline = device.createComputePipeline({
                    layout: 'auto',
                    compute: {
                        module,
                        entryPoint: 'main'
                    }
                });
                console.log('Noise Reduction pipeline created');
            } catch (error) {
                console.error('Noise Reduction pipeline creation error:', error);
                throw error;
            }
        }

        async function createIFFTPipeline() {
            const ifftShader = `
                @group(0) @binding(0) var<storage, read> input: array<vec2<f32>>;
                @group(0) @binding(1) var<storage, read_write> output: array<f32>;

                const FFT_SIZE: u32 = ${FFT_SIZE};
                const PI: f32 = 3.14159265359;

                @compute @workgroup_size(64)
                fn main(@builtin(global_invocation_id) global_id: vec3<u32>) {
                    let n = global_id.x;
                    if (n >= FFT_SIZE) {
                        return;
                    }

                    var sum_real: f32 = 0.0;
                    var sum_imag: f32 = 0.0;

                    for (var k: u32 = 0; k < FFT_SIZE; k = k + 1) {
                        let angle = 2.0 * PI * f32(k) * f32(n) / f32(FFT_SIZE);
                        let complex = input[k];

                        sum_real += complex.x * cos(angle) - complex.y * sin(angle);
                        sum_imag += complex.x * sin(angle) + complex.y * cos(angle);
                    }

                    output[n] = sum_real / f32(FFT_SIZE);
                }
            `;

            try {
                const module = device.createShaderModule({ code: ifftShader });
                console.log('IFFT shader module created');

                ifftPipeline = device.createComputePipeline({
                    layout: 'auto',
                    compute: {
                        module,
                        entryPoint: 'main'
                    }
                });
                console.log('IFFT pipeline created');
            } catch (error) {
                console.error('IFFT pipeline creation error:', error);
                throw error;
            }
        }

        async function createUpdateNoisePipeline() {
            const updateNoiseShader = `
            @group(0) @binding(0) var<storage, read_write> noiseProfile: array<f32>;
            @group(0) @binding(1) var<storage, read> fftBuffer: array<vec2<f32>>;

            const WATERFALL_WIDTH: u32 = ${WATERFALL_WIDTH};
            const ALPHA: f32 = ${ALPHA_DECAY};

            @compute @workgroup_size(64)
            fn main(@builtin(global_invocation_id) global_id: vec3<u32>) {
                let k = global_id.x;
                if (k >= WATERFALL_WIDTH) {
                    return;
                }

                let currentMagnitude = length(fftBuffer[k]);

                if (currentMagnitude < noiseProfile[k]) {
                    noiseProfile[k] = currentMagnitude;
                } else {
                    noiseProfile[k] = noiseProfile[k] * (1.0 - ALPHA) + currentMagnitude * ALPHA;
                }
            }
            `;

            try {
                const module = device.createShaderModule({ code: updateNoiseShader });
                console.log('Update Noise shader module created');

                updateNoisePipeline = device.createComputePipeline({
                    layout: 'auto',
                    compute: {
                        module,
                        entryPoint: 'main'
                    }
                });
                console.log('Update Noise pipeline created');
            } catch (error) {
                console.error('Update Noise pipeline creation error:', error);
                throw error;
            }
        }

        async function createScrollPipeline() {
            const scrollShader = `
                @group(0) @binding(0) var waterfall: texture_storage_2d<rgba8unorm, write>;
                @group(0) @binding(1) var<storage, read> magnitudes: array<vec2<f32>>;
                @group(0) @binding(2) var<uniform> params: Params;

                struct Params {
                    currentRow: u32,
                }

                const WIDTH: u32 = ${WATERFALL_WIDTH};
                const HEIGHT: u32 = ${WATERFALL_HEIGHT};

                @compute @workgroup_size(64)
                fn main(@builtin(global_invocation_id) global_id: vec3<u32>) {
                    let x = global_id.x;

                    if (x >= WIDTH) {
                        return;
                    }

                    // Write new FFT data to current row
                    let complex = magnitudes[x];
                    let magnitude = sqrt(complex.x * complex.x + complex.y * complex.y);

                    // Apply log scale for better visualization
                    // Scale up the magnitude significantly before log
                    let scaled = magnitude * 0.1;
                    let normalized = log(1.0 + scaled) / log(100.0);
                    let clamped = clamp(normalized, 0.0, 1.0);

                    let row = params.currentRow % HEIGHT;
                    // Store in red channel, green=0, blue=0, alpha=1
                    textureStore(waterfall, vec2<i32>(i32(x), i32(row)), vec4<f32>(clamped, clamped, clamped, 1.0));
                }
            `;

            try {
                const module = device.createShaderModule({ code: scrollShader });
                console.log('Scroll shader module created');

                scrollPipeline = device.createComputePipeline({
                    layout: 'auto',
                    compute: {
                        module,
                        entryPoint: 'main'
                    }
                });
                console.log('Scroll pipeline created');
            } catch (error) {
                console.error('Scroll pipeline creation error:', error);
                throw error;
            }
        }

        async function createRenderPipeline() {
            const vertexShader = `
                struct VertexOutput {
                    @builtin(position) position: vec4<f32>,
                    @location(0) uv: vec2<f32>,
                }

                @vertex
                fn main(@builtin(vertex_index) vertex_index: u32) -> VertexOutput {
                    var positions = array<vec2<f32>, 6>(
                        vec2<f32>(-1.0, -1.0),
                        vec2<f32>(1.0, -1.0),
                        vec2<f32>(-1.0, 1.0),
                        vec2<f32>(-1.0, 1.0),
                        vec2<f32>(1.0, -1.0),
                        vec2<f32>(1.0, 1.0)
                    );

                    var uvs = array<vec2<f32>, 6>(
                        vec2<f32>(0.0, 1.0),
                        vec2<f32>(1.0, 1.0),
                        vec2<f32>(0.0, 0.0),
                        vec2<f32>(0.0, 0.0),
                        vec2<f32>(1.0, 1.0),
                        vec2<f32>(1.0, 0.0)
                    );

                    var output: VertexOutput;
                    output.position = vec4<f32>(positions[vertex_index], 0.0, 1.0);
                    output.uv = uvs[vertex_index];
                    return output;
                }
            `;

            const fragmentShader = `
                @group(0) @binding(0) var waterfallTexture: texture_2d<f32>;
                @group(0) @binding(1) var waterfallSampler: sampler;
                @group(0) @binding(2) var<uniform> params: Params;

                struct Params {
                    currentRow: u32,
                }

                const HEIGHT: f32 = ${WATERFALL_HEIGHT}.0;

                @fragment
                fn main(@location(0) uv: vec2<f32>) -> @location(0) vec4<f32> {
                    // Offset UVs to create scrolling effect
                    let rowOffset = f32(params.currentRow) / HEIGHT;
                    var adjustedUV = uv;
                    adjustedUV.y = fract(uv.y + rowOffset);

                    let value = textureSample(waterfallTexture, waterfallSampler, adjustedUV).r;

                    // Color mapping: viridis-like colors
                    var color: vec3<f32>;
                    if (value < 0.25) {
                        color = mix(vec3<f32>(0.267, 0.005, 0.329), vec3<f32>(0.282, 0.141, 0.458), value * 4.0);
                    } else if (value < 0.5) {
                        color = mix(vec3<f32>(0.282, 0.141, 0.458), vec3<f32>(0.164, 0.471, 0.558), (value - 0.25) * 4.0);
                    } else if (value < 0.75) {
                        color = mix(vec3<f32>(0.164, 0.471, 0.558), vec3<f32>(0.478, 0.821, 0.318), (value - 0.5) * 4.0);
                    } else {
                        color = mix(vec3<f32>(0.478, 0.821, 0.318), vec3<f32>(0.993, 0.906, 0.144), (value - 0.75) * 4.0);
                    }

                    return vec4<f32>(color, 1.0);
                }
            `;

            try {
                const vertexModule = device.createShaderModule({ code: vertexShader });
                console.log('Vertex shader module created');

                const fragmentModule = device.createShaderModule({ code: fragmentShader });
                console.log('Fragment shader module created');

                const sampler = device.createSampler({
                    magFilter: 'linear',
                    minFilter: 'linear',
                });

                renderPipeline = device.createRenderPipeline({
                    layout: 'auto',
                    vertex: {
                        module: vertexModule,
                        entryPoint: 'main',
                    },
                    fragment: {
                        module: fragmentModule,
                        entryPoint: 'main',
                        targets: [{
                            format: navigator.gpu.getPreferredCanvasFormat(),
                        }],
                    },
                    primitive: {
                        topology: 'triangle-list',
                    },
                });
                console.log('Render pipeline created');
            } catch (error) {
                console.error('Render pipeline creation error:', error);
                throw error;
            }
        }

        async function initAudio() {
            try {
                micStream = await navigator.mediaDevices.getUserMedia({ audio: true });

                audioContext = new AudioContext({ sampleRate: 44100 });

                // Create AnalyserNode to get time-domain data
                analyserNode = audioContext.createAnalyser();
                analyserNode.fftSize = FFT_SIZE;

                // Create buffer for time-domain data
                audioDataArray = new Float32Array(FFT_SIZE);

                const source = audioContext.createMediaStreamSource(micStream);
                source.connect(analyserNode);

                console.log('Audio initialized successfully');
                console.log('Sample rate:', audioContext.sampleRate);
                console.log('Analyser FFT size:', analyserNode.fftSize);

                return true;
            } catch (error) {
                showError('Microphone access error: ' + error.message);
                return false;
            }
        }

        function processAudioData() {
            if (!device || !analyserNode) return;

            // Get time-domain data from analyser
            analyserNode.getFloatTimeDomainData(audioDataArray);

            // Store input data if recording
            if (isRecording) {
                recordedInputChunks.push(new Float32Array(audioDataArray));
            }

            // Debug: Check if we're getting audio data
            const audioLevel = Math.max(...audioDataArray.map(Math.abs));
            if (inputCurrentRow % 60 === 0) { // Log every 60 frames (~1 second)
                console.log('Audio level:', audioLevel.toFixed(4), 'Current row:', inputCurrentRow);
            }

            // Update debug display
            if (inputCurrentRow % 10 === 0) {
                const debugDiv = document.getElementById('debug');
                if (debugDiv) {
                    const status = isRecording ? 'RECORDING' : 'STOPPED';
                    debugDiv.textContent = `${status} | Audio Level: ${audioLevel.toFixed(4)} | Row: ${inputCurrentRow} | Chunks: ${recordedInputChunks.length}`;
                }
            }

            // Copy audio data to GPU buffer
            device.queue.writeBuffer(audioBuffer, 0, audioDataArray);

            // Update current row parameter
            device.queue.writeBuffer(inputParamsBuffer, 0, new Uint32Array([inputCurrentRow]));

            // Compute FFT and update input waterfall
            const commandEncoder = device.createCommandEncoder();

            // Run FFT
            const fftPass = commandEncoder.beginComputePass();
            fftPass.setPipeline(fftPipeline);
            const fftBindGroup = device.createBindGroup({
                layout: fftPipeline.getBindGroupLayout(0),
                entries: [
                    { binding: 0, resource: { buffer: audioBuffer } },
                    { binding: 1, resource: { buffer: fftBuffer } }
                ]
            });
            fftPass.setBindGroup(0, fftBindGroup);
            fftPass.dispatchWorkgroups(Math.ceil(FFT_SIZE / 64));
            fftPass.end();

            const updateNoisePass = commandEncoder.beginComputePass();
            updateNoisePass.setPipeline(updateNoisePipeline);
            const updateNoiseBindGroup = device.createBindGroup({
                layout: updateNoisePipeline.getBindGroupLayout(0),
                entries: [
                    { binding: 0, resource: { buffer: noiseProfile }},
                    { binding: 1, resource: { buffer: fftBuffer }},
                    { binding: 2, resource: { buffer: processedFFTBuffer }}
                ]
            });
            updateNoisePass.setBindGroup(0, updateNoiseBindGroup);
            updateNoisePass.dispatchWorkgroups(Math.ceil(FFT_SIZE / 64));
            updateNoisePass.end();


            const noiseReductionPass = commandEncoder.beginComputePass();
            noiseReductionPass.setPipeline(noiseReductionPipeline);
            const noiseBindGroup = device.createBindGroup({
                layout: noiseReductionPipeline.getBindGroupLayout(0),
                entries: [
                    { binding: 0, resource: { buffer: noiseProfile }},
                    { binding: 1, resource: { buffer: fftBuffer }},
                    { binding: 2, resource: { buffer: processedFFTBuffer }}
                ]
            });
            noiseReductionPass.setBindGroup(0, noiseBindGroup);
            noiseReductionPass.dispatchWorkgroups(Math.ceil(FFT_SIZE / 64));
            noiseReductionPass.end();

            // Update input waterfall texture
            const scrollPass = commandEncoder.beginComputePass();
            scrollPass.setPipeline(scrollPipeline);
            const scrollBindGroup = device.createBindGroup({
                layout: scrollPipeline.getBindGroupLayout(0),
                entries: [
                    { binding: 0, resource: inputWaterfallTexture.createView() },
                    { binding: 1, resource: { buffer: fftBuffer } },
                    { binding: 2, resource: { buffer: inputParamsBuffer } }
                ]
            });
            scrollPass.setBindGroup(0, scrollBindGroup);
            scrollPass.dispatchWorkgroups(Math.ceil(WATERFALL_WIDTH / 64));
            scrollPass.end();

            device.queue.submit([commandEncoder.finish()]);

            // Increment row for next frame
            inputCurrentRow = (inputCurrentRow + 1) % WATERFALL_HEIGHT;
        }

        let renderCount = 0;

        function render() {
            if (!device || !inputContext || !outputContext) {
                console.error('Render called but device or contexts are null');
                return;
            }

            renderCount++;

            // Process audio data each frame
            processAudioData();

            try {
                const sampler = device.createSampler({
                    magFilter: 'linear',
                    minFilter: 'linear',
                });

                // Render input canvas
                const inputEncoder = device.createCommandEncoder();
                const inputTextureView = inputContext.getCurrentTexture().createView();

                const inputRenderPass = inputEncoder.beginRenderPass({
                    colorAttachments: [{
                        view: inputTextureView,
                        loadOp: 'clear',
                        clearValue: { r: 0, g: 0, b: 0, a: 1 },
                        storeOp: 'store',
                    }],
                });

                const inputBindGroup = device.createBindGroup({
                    layout: renderPipeline.getBindGroupLayout(0),
                    entries: [
                        { binding: 0, resource: inputWaterfallTexture.createView() },
                        { binding: 1, resource: sampler },
                        { binding: 2, resource: { buffer: inputParamsBuffer } }
                    ]
                });

                inputRenderPass.setPipeline(renderPipeline);
                inputRenderPass.setBindGroup(0, inputBindGroup);
                inputRenderPass.draw(6);
                inputRenderPass.end();

                device.queue.submit([inputEncoder.finish()]);

                // Render output canvas
                const outputEncoder = device.createCommandEncoder();
                const outputTextureView = outputContext.getCurrentTexture().createView();

                const outputRenderPass = outputEncoder.beginRenderPass({
                    colorAttachments: [{
                        view: outputTextureView,
                        loadOp: 'clear',
                        clearValue: { r: 0, g: 0, b: 0, a: 1 },
                        storeOp: 'store',
                    }],
                });

                const outputBindGroup = device.createBindGroup({
                    layout: renderPipeline.getBindGroupLayout(0),
                    entries: [
                        { binding: 0, resource: outputWaterfallTexture.createView() },
                        { binding: 1, resource: sampler },
                        { binding: 2, resource: { buffer: outputParamsBuffer } }
                    ]
                });

                outputRenderPass.setPipeline(renderPipeline);
                outputRenderPass.setBindGroup(0, outputBindGroup);
                outputRenderPass.draw(6);
                outputRenderPass.end();

                device.queue.submit([outputEncoder.finish()]);
            } catch (error) {
                console.error('Render error:', error);
                isRunning = false;
            }

            if (isRunning) {
                animationId = requestAnimationFrame(render);
            }
        }

        async function start() {
            showStatus('Initializing...');

            document.getElementById('startBtn').disabled = true;

            if (!await initWebGPU()) {
                document.getElementById('startBtn').disabled = false;
                return;
            }

            if (!await initAudio()) {
                document.getElementById('startBtn').disabled = false;
                return;
            }

            // Clear previous recordings
            recordedInputChunks = [];
            recordedOutputChunks = [];
            inputAudioBuffer = null;
            outputAudioBuffer = null;
            inputCurrentRow = 0;
            outputCurrentRow = 0;

            isRunning = true;
            isRecording = true;
            document.getElementById('stopBtn').disabled = false;
            document.getElementById('processBtn').disabled = true;
            document.getElementById('playInputBtn').disabled = true;
            document.getElementById('playOutputBtn').disabled = true;
            showStatus('Recording - microphone active', true);

            render();
        }

        function stop() {
            isRunning = false;
            isRecording = false;

            if (animationId) {
                cancelAnimationFrame(animationId);
            }

            if (analyserNode) {
                analyserNode.disconnect();
                analyserNode = null;
            }

            if (audioContext) {
                audioContext.close();
                audioContext = null;
            }

            if (micStream) {
                micStream.getTracks().forEach(track => track.stop());
                micStream = null;
            }

            document.getElementById('startBtn').disabled = false;
            document.getElementById('stopBtn').disabled = true;

            // Enable process button if we have recorded data
            if (recordedInputChunks.length > 0) {
                document.getElementById('processBtn').disabled = false;
                showStatus(`Recording stopped. Captured ${recordedInputChunks.length} chunks. Click "Process Audio" to continue.`);
            } else {
                showStatus('Stopped - no data recorded');
            }
        }

        async function loadAudioFile(event) {
            const file = event.target.files[0];
            if (!file) return;

            showStatus('Loading audio file...');

            try {
                const arrayBuffer = await file.arrayBuffer();
                await processUploadedAudio(arrayBuffer, file.name);
            } catch (error) {
                showError('Error loading audio file: ' + error.message);
            }
        }

        async function loadTestFile() {
            // Get selected noise type and SNR level
            const noiseType = document.getElementById('noiseType').value;
            const snrLevel = document.getElementById('snrLevel').value;

            // Construct path: ./test-audio-dataset/mixed_noise_{type}_snr{snr}db.wav
            const TEST_FILE_PATH = `./test-audio-dataset/mixed_noise_${noiseType}_snr${snrLevel}db.wav`;

            showStatus(`Loading ${noiseType} noise at ${snrLevel}dB SNR...`);

            try {
                const response = await fetch(TEST_FILE_PATH);
                if (!response.ok) {
                    throw new Error(`Failed to load ${TEST_FILE_PATH}. Make sure you've run: python3 create-test-audio-dataset.py`);
                }

                const arrayBuffer = await response.arrayBuffer();
                await processUploadedAudio(arrayBuffer, `${noiseType}_${snrLevel}dB`);
            } catch (error) {
                showError('Error loading test file: ' + error.message);
                console.error('Test file error:', error);
            }
        }

        async function processUploadedAudio(arrayBuffer, filename) {
            if (!await initWebGPU()) {
                return;
            }

            // Decode audio data
            const audioContext = new AudioContext({ sampleRate: SAMPLE_RATE });
            const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);

            console.log('Loaded audio:', {
                duration: audioBuffer.duration,
                sampleRate: audioBuffer.sampleRate,
                channels: audioBuffer.numberOfChannels,
                length: audioBuffer.length
            });

            // Get audio data (mono - mix channels if stereo)
            let audioData;
            if (audioBuffer.numberOfChannels === 1) {
                audioData = audioBuffer.getChannelData(0);
            } else {
                // Mix to mono
                const left = audioBuffer.getChannelData(0);
                const right = audioBuffer.getChannelData(1);
                audioData = new Float32Array(left.length);
                for (let i = 0; i < left.length; i++) {
                    audioData[i] = (left[i] + right[i]) / 2;
                }
            }

            // Resample if needed (simple decimation/interpolation)
            if (audioBuffer.sampleRate !== SAMPLE_RATE) {
                console.log(`Resampling from ${audioBuffer.sampleRate} Hz to ${SAMPLE_RATE} Hz`);
                const ratio = SAMPLE_RATE / audioBuffer.sampleRate;
                const newLength = Math.floor(audioData.length * ratio);
                const resampled = new Float32Array(newLength);

                for (let i = 0; i < newLength; i++) {
                    const srcIndex = i / ratio;
                    const srcIndexFloor = Math.floor(srcIndex);
                    const srcIndexCeil = Math.min(srcIndexFloor + 1, audioData.length - 1);
                    const fraction = srcIndex - srcIndexFloor;

                    // Linear interpolation
                    resampled[i] = audioData[srcIndexFloor] * (1 - fraction) + audioData[srcIndexCeil] * fraction;
                }

                audioData = resampled;
            }

            // Split into chunks
            recordedInputChunks = [];
            for (let i = 0; i < audioData.length; i += FFT_SIZE) {
                const chunk = audioData.slice(i, i + FFT_SIZE);
                if (chunk.length === FFT_SIZE) {
                    recordedInputChunks.push(chunk);
                }
            }

            console.log(`Split audio into ${recordedInputChunks.length} chunks`);

            // Visualize the input
            await visualizeInput();

            // Enable process button
            document.getElementById('processBtn').disabled = false;
            document.getElementById('playInputBtn').disabled = false;
            showStatus(`Loaded ${filename} (${audioBuffer.duration.toFixed(2)}s, ${recordedInputChunks.length} chunks). Click "Process Audio" to continue.`, true);

            await audioContext.close();
        }

        async function visualizeInput() {
            if (!device || recordedInputChunks.length === 0) return;

            showStatus('Generating input waterfall...');
            inputCurrentRow = 0;

            for (const chunk of recordedInputChunks) {
                // Copy audio data to GPU buffer
                device.queue.writeBuffer(audioBuffer, 0, chunk);
                device.queue.writeBuffer(inputParamsBuffer, 0, new Uint32Array([inputCurrentRow]));

                // Compute FFT and update input waterfall
                const commandEncoder = device.createCommandEncoder();

                // Run FFT
                const fftPass = commandEncoder.beginComputePass();
                fftPass.setPipeline(fftPipeline);
                const fftBindGroup = device.createBindGroup({
                    layout: fftPipeline.getBindGroupLayout(0),
                    entries: [
                        { binding: 0, resource: { buffer: audioBuffer } },
                        { binding: 1, resource: { buffer: fftBuffer } }
                    ]
                });
                fftPass.setBindGroup(0, fftBindGroup);
                fftPass.dispatchWorkgroups(Math.ceil(FFT_SIZE / 64));
                fftPass.end();

                // Update input waterfall texture
                const scrollPass = commandEncoder.beginComputePass();
                scrollPass.setPipeline(scrollPipeline);
                const scrollBindGroup = device.createBindGroup({
                    layout: scrollPipeline.getBindGroupLayout(0),
                    entries: [
                        { binding: 0, resource: inputWaterfallTexture.createView() },
                        { binding: 1, resource: { buffer: fftBuffer } },
                        { binding: 2, resource: { buffer: inputParamsBuffer } }
                    ]
                });
                scrollPass.setBindGroup(0, scrollBindGroup);
                scrollPass.dispatchWorkgroups(Math.ceil(WATERFALL_WIDTH / 64));
                scrollPass.end();

                device.queue.submit([commandEncoder.finish()]);

                inputCurrentRow = (inputCurrentRow + 1) % WATERFALL_HEIGHT;
            }

            // Render input canvas one final time
            const sampler = device.createSampler({
                magFilter: 'linear',
                minFilter: 'linear',
            });

            const inputEncoder = device.createCommandEncoder();
            const inputTextureView = inputContext.getCurrentTexture().createView();

            const inputRenderPass = inputEncoder.beginRenderPass({
                colorAttachments: [{
                    view: inputTextureView,
                    loadOp: 'clear',
                    clearValue: { r: 0, g: 0, b: 0, a: 1 },
                    storeOp: 'store',
                }],
            });

            const inputBindGroup = device.createBindGroup({
                layout: renderPipeline.getBindGroupLayout(0),
                entries: [
                    { binding: 0, resource: inputWaterfallTexture.createView() },
                    { binding: 1, resource: sampler },
                    { binding: 2, resource: { buffer: inputParamsBuffer } }
                ]
            });

            inputRenderPass.setPipeline(renderPipeline);
            inputRenderPass.setBindGroup(0, inputBindGroup);
            inputRenderPass.draw(6);
            inputRenderPass.end();

            device.queue.submit([inputEncoder.finish()]);
        }

        // GPU-BASED AUDIO PROCESSING
        async function processAudioBuffer(inputData) {
            console.log('=== GPU NOISE REDUCTION START ===');
            console.log(`Parameters: Noise Gain=${NOISE_GAIN}, Signal Gain=${SIGNAL_GAIN}, Alpha=${ALPHA_DECAY}`);

            // Split input into chunks
            const chunks = [];
            for (let i = 0; i < inputData.length; i += FFT_SIZE) {
                const chunk = inputData.slice(i, i + FFT_SIZE);
                if (chunk.length === FFT_SIZE) {
                    chunks.push(chunk);
                }
            }

            const processedChunks = [];

            // Process each chunk through the GPU pipeline
            for (let i = 0; i < chunks.length; i++) {
                const chunk = chunks[i];

                // Upload chunk to GPU
                device.queue.writeBuffer(audioBuffer, 0, chunk);

                // Create command encoder for this chunk
                const commandEncoder = device.createCommandEncoder();

                // 1. FFT: time → frequency
                const fftPass = commandEncoder.beginComputePass();
                fftPass.setPipeline(fftPipeline);
                const fftBindGroup = device.createBindGroup({
                    label: 'fftBindGroup',
                    layout: fftPipeline.getBindGroupLayout(0),
                    entries: [
                        { binding: 0, resource: { buffer: audioBuffer } },
                        { binding: 1, resource: { buffer: fftBuffer } }
                    ]
                });
                fftPass.setBindGroup(0, fftBindGroup);
                fftPass.dispatchWorkgroups(Math.ceil(FFT_SIZE / 64));
                fftPass.end();

                // 2. Update noise profile (minimum statistics)
                const noiseUpdatePass = commandEncoder.beginComputePass();
                noiseUpdatePass.setPipeline(updateNoisePipeline);
                const noiseUpdateBindGroup = device.createBindGroup({
                    label: 'noiseUpdateBindGroup',
                    layout: updateNoisePipeline.getBindGroupLayout(0),
                    entries: [
                        { binding: 0, resource: { buffer: noiseProfile } },
                        { binding: 1, resource: { buffer: fftBuffer } },
                    ]
                });
                noiseUpdatePass.setBindGroup(0, noiseUpdateBindGroup);
                noiseUpdatePass.dispatchWorkgroups(Math.ceil(WATERFALL_WIDTH / 64));
                noiseUpdatePass.end();

                // 3. Noise reduction: apply noise profile
                const noiseReductionPass = commandEncoder.beginComputePass();
                noiseReductionPass.setPipeline(noiseReductionPipeline);
                const noiseReductionBindGroup = device.createBindGroup({
                    label: 'noiseReductionBindGroup',
                    layout: noiseReductionPipeline.getBindGroupLayout(0),
                    entries: [
                        { binding: 0, resource: { buffer: noiseProfile } },
                        { binding: 1, resource: { buffer: fftBuffer } },
                        { binding: 2, resource: { buffer: processedFFTBuffer } }
                    ]
                });
                noiseReductionPass.setBindGroup(0, noiseReductionBindGroup);
                noiseReductionPass.dispatchWorkgroups(Math.ceil(FFT_SIZE / 64));
                noiseReductionPass.end();

                // 4. IFFT: frequency → time
                const ifftPass = commandEncoder.beginComputePass();
                ifftPass.setPipeline(ifftPipeline);
                const ifftBindGroup = device.createBindGroup({
                    label: 'iFFTBindGroup',
                    layout: ifftPipeline.getBindGroupLayout(0),
                    entries: [
                        { binding: 0, resource: { buffer: processedFFTBuffer } },
                        { binding: 1, resource: { buffer: processedAudioBuffer } }
                    ]
                });
                ifftPass.setBindGroup(0, ifftBindGroup);
                ifftPass.dispatchWorkgroups(Math.ceil(FFT_SIZE / 64));
                ifftPass.end();

                // Submit GPU commands
                device.queue.submit([commandEncoder.finish()]);

                // Read back processed audio from GPU
                const readBuffer = device.createBuffer({
                    size: FFT_SIZE * 4,
                    usage: GPUBufferUsage.COPY_DST | GPUBufferUsage.MAP_READ
                });

                const copyEncoder = device.createCommandEncoder();
                copyEncoder.copyBufferToBuffer(processedAudioBuffer, 0, readBuffer, 0, FFT_SIZE * 4);
                device.queue.submit([copyEncoder.finish()]);

                // Wait for GPU to finish and map the buffer
                await readBuffer.mapAsync(GPUMapMode.READ);
                const processedChunk = new Float32Array(readBuffer.getMappedRange()).slice();
                readBuffer.unmap();

                processedChunks.push(processedChunk);

                // Progress update
                // if (i % 10 === 0) {
                //     console.log(`Processed ${i}/${chunks.length} chunks`);
                // }

                // Debug: Check noise profile and FFT magnitudes at multiple time points
                // At 44100 Hz with 1024 chunk size: ~215 chunks = 5sec, ~258 = 6sec, ~301 = 7sec
                if (i === 215 || i === 258 || i === 301) {
                    const timeSeconds = (i * FFT_SIZE / SAMPLE_RATE).toFixed(1);
                    console.log(`\n=== DIAGNOSTIC at ${timeSeconds}s (chunk ${i}) ===`);
                    setTimeout(async () => {
                        await debugNoiseProfile();

                        // Also read back FFT buffer to compare magnitudes
                        const fftReadBuffer = device.createBuffer({
                            size: WATERFALL_WIDTH * 2 * 4, // Complex numbers
                            usage: GPUBufferUsage.COPY_DST | GPUBufferUsage.MAP_READ
                        });
                        const cmdEnc = device.createCommandEncoder();
                        cmdEnc.copyBufferToBuffer(fftBuffer, 0, fftReadBuffer, 0, WATERFALL_WIDTH * 2 * 4);
                        device.queue.submit([cmdEnc.finish()]);
                        await fftReadBuffer.mapAsync(GPUMapMode.READ);
                        const fftData = new Float32Array(fftReadBuffer.getMappedRange());

                        // Calculate magnitudes
                        const magnitudes = [];
                        for (let k = 0; k < WATERFALL_WIDTH; k++) {
                            const real = fftData[k * 2];
                            const imag = fftData[k * 2 + 1];
                            magnitudes.push(Math.sqrt(real * real + imag * imag));
                        }

                        console.log('FFT MAGNITUDES:');
                        console.log('  Bins 0-9 (0-430 Hz):', magnitudes.slice(0, 10));
                        console.log('  Bins 10-19 (430-860 Hz - speech):', magnitudes.slice(10, 20));
                        console.log('  Bins 30-39 (1290-1720 Hz - speech):', magnitudes.slice(30, 40));
                        console.log('  Overall range:', Math.min(...magnitudes).toFixed(3), '-', Math.max(...magnitudes).toFixed(3));
                        console.log('  Speech range (bins 10-50):',
                            Math.min(...magnitudes.slice(10, 50)).toFixed(3), '-', Math.max(...magnitudes.slice(10, 50)).toFixed(3));

                        fftReadBuffer.unmap();
                    }, 100);
                }
            }

            // Concatenate all processed chunks
            const totalSamples = processedChunks.reduce((sum, chunk) => sum + chunk.length, 0);
            const outputData = new Float32Array(totalSamples);
            let offset = 0;
            for (const chunk of processedChunks) {
                outputData.set(chunk, offset);
                offset += chunk.length;
            }

            console.log('=== GPU NOISE REDUCTION COMPLETE ===');
            console.log(`Processed ${chunks.length} chunks`);

            // Log sample comparison
            const inputSample = inputData.slice(0, 10);
            const outputSample = outputData.slice(0, 10);
            console.log('Input sample:', inputSample);
            console.log('Output sample:', outputSample);
            console.log('Difference:', outputSample.map((v, i) => Math.abs(v - inputSample[i])));

            // Calculate RMS to detect if processing is working
            const inputRMS = Math.sqrt(inputData.reduce((sum, v) => sum + v*v, 0) / inputData.length);
            const outputRMS = Math.sqrt(outputData.reduce((sum, v) => sum + v*v, 0) / outputData.length);
            console.log(`Input RMS: ${inputRMS.toFixed(6)}, Output RMS: ${outputRMS.toFixed(6)}, Ratio: ${(outputRMS/inputRMS).toFixed(3)}`);

            if (Math.abs(outputRMS - inputRMS) < 0.0001) {
                console.warn('⚠️ WARNING: Output is nearly identical to input! Noise reduction may not be working.');
            }

            return outputData;
        }

        async function processRecording() {
            if (recordedInputChunks.length === 0) {
                showError('No recorded data to process');
                return;
            }

            showStatus('Processing audio...');
            document.getElementById('processBtn').disabled = true;

            // Concatenate all input chunks into a single buffer
            const totalSamples = recordedInputChunks.reduce((sum, chunk) => sum + chunk.length, 0);
            const inputData = new Float32Array(totalSamples);
            let offset = 0;
            for (const chunk of recordedInputChunks) {
                inputData.set(chunk, offset);
                offset += chunk.length;
            }

            console.log(`Processing ${totalSamples} samples (${(totalSamples / SAMPLE_RATE).toFixed(2)} seconds)`);

            // Process the audio through GPU pipeline
            const outputData = await processAudioBuffer(inputData);

            // Split output back into chunks for visualization
            recordedOutputChunks = [];
            for (let i = 0; i < outputData.length; i += FFT_SIZE) {
                const chunk = outputData.slice(i, i + FFT_SIZE);
                if (chunk.length === FFT_SIZE) {
                    recordedOutputChunks.push(chunk);
                }
            }

            // Visualize the output
            await visualizeOutput();

            // Enable playback and re-enable process button for re-processing
            document.getElementById('processBtn').disabled = false;
            document.getElementById('playInputBtn').disabled = false;
            document.getElementById('playOutputBtn').disabled = false;
            showStatus('Processing complete. You can now play input/output or re-process with different parameters.', true);
        }

        async function visualizeOutput() {
            if (!device || recordedOutputChunks.length === 0) return;

            showStatus('Generating output waterfall...');
            outputCurrentRow = 0;

            for (const chunk of recordedOutputChunks) {
                // Copy audio data to GPU buffer
                device.queue.writeBuffer(audioBuffer, 0, chunk);
                device.queue.writeBuffer(outputParamsBuffer, 0, new Uint32Array([outputCurrentRow]));

                // Compute FFT and update output waterfall
                const commandEncoder = device.createCommandEncoder();

                // Run FFT
                const fftPass = commandEncoder.beginComputePass();
                fftPass.setPipeline(fftPipeline);
                const fftBindGroup = device.createBindGroup({
                    layout: fftPipeline.getBindGroupLayout(0),
                    entries: [
                        { binding: 0, resource: { buffer: audioBuffer } },
                        { binding: 1, resource: { buffer: fftBuffer } }
                    ]
                });
                fftPass.setBindGroup(0, fftBindGroup);
                fftPass.dispatchWorkgroups(Math.ceil(FFT_SIZE / 64));
                fftPass.end();

                // Update output waterfall texture
                const scrollPass = commandEncoder.beginComputePass();
                scrollPass.setPipeline(scrollPipeline);
                const scrollBindGroup = device.createBindGroup({
                    layout: scrollPipeline.getBindGroupLayout(0),
                    entries: [
                        { binding: 0, resource: outputWaterfallTexture.createView() },
                        { binding: 1, resource: { buffer: fftBuffer } },
                        { binding: 2, resource: { buffer: outputParamsBuffer } }
                    ]
                });
                scrollPass.setBindGroup(0, scrollBindGroup);
                scrollPass.dispatchWorkgroups(Math.ceil(WATERFALL_WIDTH / 64));
                scrollPass.end();

                device.queue.submit([commandEncoder.finish()]);

                outputCurrentRow = (outputCurrentRow + 1) % WATERFALL_HEIGHT;
            }

            // Render both canvases one final time
            const sampler = device.createSampler({
                magFilter: 'linear',
                minFilter: 'linear',
            });

            // Render output canvas
            const outputEncoder = device.createCommandEncoder();
            const outputTextureView = outputContext.getCurrentTexture().createView();

            const outputRenderPass = outputEncoder.beginRenderPass({
                colorAttachments: [{
                    view: outputTextureView,
                    loadOp: 'clear',
                    clearValue: { r: 0, g: 0, b: 0, a: 1 },
                    storeOp: 'store',
                }],
            });

            const outputBindGroup = device.createBindGroup({
                layout: renderPipeline.getBindGroupLayout(0),
                entries: [
                    { binding: 0, resource: outputWaterfallTexture.createView() },
                    { binding: 1, resource: sampler },
                    { binding: 2, resource: { buffer: outputParamsBuffer } }
                ]
            });

            outputRenderPass.setPipeline(renderPipeline);
            outputRenderPass.setBindGroup(0, outputBindGroup);
            outputRenderPass.draw(6);
            outputRenderPass.end();

            device.queue.submit([outputEncoder.finish()]);
        }

        function playInput() {
            playAudioBuffer(recordedInputChunks, 'input');
        }

        function playOutput() {
            playAudioBuffer(recordedOutputChunks, 'output');
        }

        function playAudioBuffer(chunks, label) {
            if (chunks.length === 0) {
                showError(`No ${label} data to play`);
                return;
            }

            // Concatenate chunks
            const totalSamples = chunks.reduce((sum, chunk) => sum + chunk.length, 0);
            const audioData = new Float32Array(totalSamples);
            let offset = 0;
            for (const chunk of chunks) {
                audioData.set(chunk, offset);
                offset += chunk.length;
            }

            // Create audio context and buffer
            const playbackContext = new AudioContext({ sampleRate: SAMPLE_RATE });
            const buffer = playbackContext.createBuffer(1, audioData.length, SAMPLE_RATE);
            buffer.copyToChannel(audioData, 0);

            // Play
            const source = playbackContext.createBufferSource();
            source.buffer = buffer;
            source.connect(playbackContext.destination);
            source.start();

            const duration = audioData.length / SAMPLE_RATE;
            showStatus(`Playing ${label} (${duration.toFixed(2)}s)...`, true);

            source.onended = () => {
                playbackContext.close();
                showStatus(`${label} playback finished`);
            };
        }

        function updateParamDisplay() {
            const noiseGain = parseFloat(document.getElementById('noiseGain').value);
            const signalGain = parseFloat(document.getElementById('signalGain').value);
            const alphaDecay = parseFloat(document.getElementById('alphaDecay').value);

            document.getElementById('noiseGainValue').textContent = noiseGain.toFixed(2);
            document.getElementById('signalGainValue').textContent = signalGain.toFixed(2);
            document.getElementById('alphaDecayValue').textContent = alphaDecay.toFixed(3);
        }

        async function applyParameters() {
            if (!device) {
                showError('Please initialize WebGPU first (load a file or start recording)');
                return;
            }

            // Read values from UI
            NOISE_GAIN = parseFloat(document.getElementById('noiseGain').value);
            SIGNAL_GAIN = parseFloat(document.getElementById('signalGain').value);
            ALPHA_DECAY = parseFloat(document.getElementById('alphaDecay').value);

            showStatus('Applying parameters and recreating pipelines...');

            try {
                // Recreate pipelines with new parameter values
                await createNoiseReductionPipeline();
                await createUpdateNoisePipeline();

                showStatus(`Parameters applied! Noise Gain: ${NOISE_GAIN.toFixed(2)}, Signal Gain: ${SIGNAL_GAIN.toFixed(2)}, Alpha: ${ALPHA_DECAY.toFixed(3)}`, true);
            } catch (error) {
                showError('Error applying parameters: ' + error.message);
                console.error('Parameter application error:', error);
            }
        }

        function resetParameters() {
            // Reset to defaults
            document.getElementById('noiseGain').value = 0.2;
            document.getElementById('signalGain').value = 1.0;
            document.getElementById('alphaDecay').value = 0.01;

            updateParamDisplay();
            showStatus('Parameters reset to defaults. Click "Apply Parameters" to use them.');
        }

        async function debugNoiseProfile() {
            if (!device || !noiseProfile) {
                console.error('Device or noise profile not initialized');
                return;
            }

            // Read back noise profile from GPU
            const readBuffer = device.createBuffer({
                size: WATERFALL_WIDTH * 4,
                usage: GPUBufferUsage.COPY_DST | GPUBufferUsage.MAP_READ
            });

            const commandEncoder = device.createCommandEncoder();
            commandEncoder.copyBufferToBuffer(noiseProfile, 0, readBuffer, 0, WATERFALL_WIDTH * 4);
            device.queue.submit([commandEncoder.finish()]);

            await readBuffer.mapAsync(GPUMapMode.READ);
            const data = new Float32Array(readBuffer.getMappedRange());

            console.log('NOISE PROFILE:');
            console.log('  Bins 0-9:', Array.from(data.slice(0, 10)).map(v => v.toFixed(3)));
            console.log('  Bins 10-19:', Array.from(data.slice(10, 20)).map(v => v.toFixed(3)));
            console.log('  Range:', Math.min(...data).toFixed(3), '-', Math.max(...data).toFixed(3));
            console.log('  Average:', (data.reduce((a, b) => a + b, 0) / data.length).toFixed(3));

            readBuffer.unmap();
        }

        function showStatus(message, isSuccess = false) {
            const statusDiv = document.getElementById('status');
            statusDiv.textContent = message;
            statusDiv.style.color = isSuccess ? '#22c55e' : '#fff';
            statusDiv.className = 'status';
        }

        function showError(message) {
            const statusDiv = document.getElementById('status');
            statusDiv.innerHTML = `<div class="error">${message}</div>`;
        }
    </script>
</body>
</html>
