<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>WebGPU Audio Spectrum Waterfall</title>
    <style>
        body {
            font-family: system-ui, -apple-system, sans-serif;
            max-width: 1200px;
            margin: 40px auto;
            padding: 0 20px;
            background: #1a1a1a;
            color: #fff;
        }
        h1 {
            color: #fff;
        }
        .controls {
            background: #2a2a2a;
            padding: 20px;
            border-radius: 8px;
            margin: 20px 0;
        }
        button {
            padding: 12px 24px;
            font-size: 16px;
            background: #3b82f6;
            color: white;
            border: none;
            border-radius: 6px;
            cursor: pointer;
            font-weight: 500;
            margin-right: 10px;
        }
        button:hover:not(:disabled) {
            background: #2563eb;
        }
        button:disabled {
            background: #9ca3af;
            cursor: not-allowed;
        }
        button.stop {
            background: #ef4444;
        }
        button.stop:hover:not(:disabled) {
            background: #dc2626;
        }
        .canvas-container {
            display: flex;
            gap: 20px;
            justify-content: center;
            margin: 20px 0;
        }
        .canvas-wrapper {
            flex: 1;
            max-width: 512px;
        }
        .canvas-wrapper h3 {
            text-align: center;
            margin: 0 0 10px 0;
            color: #9ca3af;
            font-size: 14px;
        }
        canvas {
            display: block;
            width: 100%;
            height: 512px;
            background: #000;
            border-radius: 8px;
        }
        .status {
            padding: 10px;
            background: #374151;
            border-radius: 4px;
            margin-top: 10px;
        }
        .error {
            color: #ef4444;
            background: #fee;
            padding: 10px;
            border-radius: 4px;
            margin: 10px 0;
        }
        .info {
            background: #2a2a2a;
            padding: 15px;
            border-radius: 8px;
            margin: 20px 0;
            font-size: 14px;
        }
    </style>
</head>
<body>
    <h1>WebGPU Audio Spectrum Waterfall</h1>

    <div class="info">
        <strong>Dual Waterfall - Input/Output Comparison</strong><br>
        1. Click "Start Recording" to capture audio from your microphone<br>
        2. Click "Stop" when done recording<br>
        3. Click "Process Audio" to run your audio processing algorithm (currently just copies input to output)<br>
        4. Use "Play Input" and "Play Output" to compare the audio before and after processing<br>
        <strong>Frequency range: 0 - 4,000 Hz</strong> (optimized for human speech) | X-axis: Frequency bins | Y-axis: Time scrolling down
    </div>

    <div class="controls">
        <button id="startBtn" onclick="start()">Start Recording</button>
        <button id="stopBtn" onclick="stop()" class="stop" disabled>Stop</button>
        <button id="processBtn" onclick="processRecording()" disabled>Process Audio</button>
        <button id="playInputBtn" onclick="playInput()" disabled>Play Input</button>
        <button id="playOutputBtn" onclick="playOutput()" disabled>Play Output</button>
        <div id="status" class="status">Click "Start Recording" to begin</div>
        <div id="debug" style="margin-top: 10px; font-family: monospace; font-size: 12px;"></div>
    </div>

    <div class="canvas-container">
        <div class="canvas-wrapper">
            <h3>INPUT</h3>
            <canvas id="inputCanvas" width="512" height="512"></canvas>
        </div>
        <div class="canvas-wrapper">
            <h3>OUTPUT (Processed)</h3>
            <canvas id="outputCanvas" width="512" height="512"></canvas>
        </div>
    </div>

    <script>
        const FFT_SIZE = 1024;
        const WATERFALL_HEIGHT = 512;
        const SAMPLE_RATE = 44100;

        // Focus on human speech range: 0-8000 Hz instead of full 0-22050 Hz
        const MAX_DISPLAY_FREQ = 4000; // Hz
        const FREQ_PER_BIN = SAMPLE_RATE / FFT_SIZE; // ~86 Hz per bin
        const WATERFALL_WIDTH = Math.floor(MAX_DISPLAY_FREQ / FREQ_PER_BIN); // ~186 bins for 0-8kHz

        let device = null;
        let inputContext = null;
        let outputContext = null;
        let renderPipeline = null;
        let fftPipeline = null;
        let scrollPipeline = null;

        let audioContext = null;
        let analyserNode = null;
        let micStream = null;
        let audioDataArray = null;

        let inputWaterfallTexture = null;
        let outputWaterfallTexture = null;
        let audioBuffer = null;
        let fftBuffer = null;
        let inputParamsBuffer = null;
        let outputParamsBuffer = null;

        let animationId = null;
        let isRunning = false;
        let isRecording = false;
        let inputCurrentRow = 0;
        let outputCurrentRow = 0;

        // Recording storage
        let recordedInputChunks = [];
        let recordedOutputChunks = [];
        let inputAudioBuffer = null;
        let outputAudioBuffer = null;

        async function initWebGPU() {
            if (device) return true;

            if (!navigator.gpu) {
                showError('WebGPU is not supported in this browser');
                return false;
            }

            try {
                const adapter = await navigator.gpu.requestAdapter();
                if (!adapter) {
                    showError('Failed to get GPU adapter');
                    return false;
                }

                device = await adapter.requestDevice();

                const inputCanvas = document.getElementById('inputCanvas');
                const outputCanvas = document.getElementById('outputCanvas');
                inputContext = inputCanvas.getContext('webgpu');
                outputContext = outputCanvas.getContext('webgpu');

                const presentationFormat = navigator.gpu.getPreferredCanvasFormat();
                inputContext.configure({
                    device,
                    format: presentationFormat,
                });
                outputContext.configure({
                    device,
                    format: presentationFormat,
                });

                await setupWebGPUResources();

                console.log('WebGPU initialized successfully');
                console.log('Waterfall texture:', WATERFALL_WIDTH, 'x', WATERFALL_HEIGHT);
                console.log('FFT size:', FFT_SIZE);

                return true;
            } catch (error) {
                showError('WebGPU initialization error: ' + error.message);
                return false;
            }
        }

        async function setupWebGPUResources() {
            // Create waterfall textures for input and output (width = freq bins, height = time)
            // Use rgba8unorm instead of r8unorm because it supports storage writes
            inputWaterfallTexture = device.createTexture({
                size: [WATERFALL_WIDTH, WATERFALL_HEIGHT],
                format: 'rgba8unorm',
                usage: GPUTextureUsage.TEXTURE_BINDING |
                       GPUTextureUsage.STORAGE_BINDING |
                       GPUTextureUsage.COPY_DST
            });

            outputWaterfallTexture = device.createTexture({
                size: [WATERFALL_WIDTH, WATERFALL_HEIGHT],
                format: 'rgba8unorm',
                usage: GPUTextureUsage.TEXTURE_BINDING |
                       GPUTextureUsage.STORAGE_BINDING |
                       GPUTextureUsage.COPY_DST
            });

            // Create buffers for audio data and FFT results
            audioBuffer = device.createBuffer({
                size: FFT_SIZE * 4, // Float32
                usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST
            });

            fftBuffer = device.createBuffer({
                size: FFT_SIZE * 2 * 4, // Complex numbers (real, imag)
                usage: GPUBufferUsage.STORAGE
            });

            inputParamsBuffer = device.createBuffer({
                size: 4, // u32 for current row
                usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST
            });

            outputParamsBuffer = device.createBuffer({
                size: 4, // u32 for current row
                usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST
            });

            // Initialize textures with black
            const blackData = new Uint8Array(WATERFALL_WIDTH * WATERFALL_HEIGHT * 4); // RGBA
            for (let i = 0; i < blackData.length; i += 4) {
                blackData[i + 3] = 255; // Alpha = 1
            }
            device.queue.writeTexture(
                { texture: inputWaterfallTexture },
                blackData,
                { bytesPerRow: WATERFALL_WIDTH * 4 },
                { width: WATERFALL_WIDTH, height: WATERFALL_HEIGHT }
            );
            device.queue.writeTexture(
                { texture: outputWaterfallTexture },
                blackData,
                { bytesPerRow: WATERFALL_WIDTH * 4 },
                { width: WATERFALL_WIDTH, height: WATERFALL_HEIGHT }
            );
            console.log('Textures initialized');

            // Create pipelines
            await createFFTPipeline();
            await createScrollPipeline();
            await createRenderPipeline();
        }

        async function createFFTPipeline() {
            // Simple DFT for now (will be slow but functional)
            // TODO: Implement Cooley-Tukey FFT for better performance
            const fftShader = `
                @group(0) @binding(0) var<storage, read> input: array<f32>;
                @group(0) @binding(1) var<storage, read_write> output: array<vec2<f32>>;

                const FFT_SIZE: u32 = ${FFT_SIZE};
                const PI: f32 = 3.14159265359;

                @compute @workgroup_size(64)
                fn main(@builtin(global_invocation_id) global_id: vec3<u32>) {
                    let k = global_id.x;
                    if (k >= FFT_SIZE) {
                        return;
                    }

                    var sum_real: f32 = 0.0;
                    var sum_imag: f32 = 0.0;

                    for (var n: u32 = 0; n < FFT_SIZE; n = n + 1) {
                        let angle = -2.0 * PI * f32(k) * f32(n) / f32(FFT_SIZE);
                        sum_real = sum_real + input[n] * cos(angle);
                        sum_imag = sum_imag + input[n] * sin(angle);
                    }

                    output[k] = vec2<f32>(sum_real, sum_imag);
                }
            `;

            try {
                const module = device.createShaderModule({ code: fftShader });
                console.log('FFT shader module created');

                fftPipeline = device.createComputePipeline({
                    layout: 'auto',
                    compute: {
                        module,
                        entryPoint: 'main'
                    }
                });
                console.log('FFT pipeline created');
            } catch (error) {
                console.error('FFT pipeline creation error:', error);
                throw error;
            }
        }

        async function createScrollPipeline() {
            const scrollShader = `
                @group(0) @binding(0) var waterfall: texture_storage_2d<rgba8unorm, write>;
                @group(0) @binding(1) var<storage, read> magnitudes: array<vec2<f32>>;
                @group(0) @binding(2) var<uniform> params: Params;

                struct Params {
                    currentRow: u32,
                }

                const WIDTH: u32 = ${WATERFALL_WIDTH};
                const HEIGHT: u32 = ${WATERFALL_HEIGHT};

                @compute @workgroup_size(64)
                fn main(@builtin(global_invocation_id) global_id: vec3<u32>) {
                    let x = global_id.x;

                    if (x >= WIDTH) {
                        return;
                    }

                    // Write new FFT data to current row
                    let complex = magnitudes[x];
                    let magnitude = sqrt(complex.x * complex.x + complex.y * complex.y);

                    // Apply log scale for better visualization
                    // Scale up the magnitude significantly before log
                    let scaled = magnitude * 0.1;
                    let normalized = log(1.0 + scaled) / log(100.0);
                    let clamped = clamp(normalized, 0.0, 1.0);

                    let row = params.currentRow % HEIGHT;
                    // Store in red channel, green=0, blue=0, alpha=1
                    textureStore(waterfall, vec2<i32>(i32(x), i32(row)), vec4<f32>(clamped, clamped, clamped, 1.0));
                }
            `;

            try {
                const module = device.createShaderModule({ code: scrollShader });
                console.log('Scroll shader module created');

                scrollPipeline = device.createComputePipeline({
                    layout: 'auto',
                    compute: {
                        module,
                        entryPoint: 'main'
                    }
                });
                console.log('Scroll pipeline created');
            } catch (error) {
                console.error('Scroll pipeline creation error:', error);
                throw error;
            }
        }

        async function createRenderPipeline() {
            const vertexShader = `
                struct VertexOutput {
                    @builtin(position) position: vec4<f32>,
                    @location(0) uv: vec2<f32>,
                }

                @vertex
                fn main(@builtin(vertex_index) vertex_index: u32) -> VertexOutput {
                    var positions = array<vec2<f32>, 6>(
                        vec2<f32>(-1.0, -1.0),
                        vec2<f32>(1.0, -1.0),
                        vec2<f32>(-1.0, 1.0),
                        vec2<f32>(-1.0, 1.0),
                        vec2<f32>(1.0, -1.0),
                        vec2<f32>(1.0, 1.0)
                    );

                    var uvs = array<vec2<f32>, 6>(
                        vec2<f32>(0.0, 1.0),
                        vec2<f32>(1.0, 1.0),
                        vec2<f32>(0.0, 0.0),
                        vec2<f32>(0.0, 0.0),
                        vec2<f32>(1.0, 1.0),
                        vec2<f32>(1.0, 0.0)
                    );

                    var output: VertexOutput;
                    output.position = vec4<f32>(positions[vertex_index], 0.0, 1.0);
                    output.uv = uvs[vertex_index];
                    return output;
                }
            `;

            const fragmentShader = `
                @group(0) @binding(0) var waterfallTexture: texture_2d<f32>;
                @group(0) @binding(1) var waterfallSampler: sampler;
                @group(0) @binding(2) var<uniform> params: Params;

                struct Params {
                    currentRow: u32,
                }

                const HEIGHT: f32 = ${WATERFALL_HEIGHT}.0;

                @fragment
                fn main(@location(0) uv: vec2<f32>) -> @location(0) vec4<f32> {
                    // Offset UVs to create scrolling effect
                    let rowOffset = f32(params.currentRow) / HEIGHT;
                    var adjustedUV = uv;
                    adjustedUV.y = fract(uv.y + rowOffset);

                    let value = textureSample(waterfallTexture, waterfallSampler, adjustedUV).r;

                    // Color mapping: viridis-like colors
                    var color: vec3<f32>;
                    if (value < 0.25) {
                        color = mix(vec3<f32>(0.267, 0.005, 0.329), vec3<f32>(0.282, 0.141, 0.458), value * 4.0);
                    } else if (value < 0.5) {
                        color = mix(vec3<f32>(0.282, 0.141, 0.458), vec3<f32>(0.164, 0.471, 0.558), (value - 0.25) * 4.0);
                    } else if (value < 0.75) {
                        color = mix(vec3<f32>(0.164, 0.471, 0.558), vec3<f32>(0.478, 0.821, 0.318), (value - 0.5) * 4.0);
                    } else {
                        color = mix(vec3<f32>(0.478, 0.821, 0.318), vec3<f32>(0.993, 0.906, 0.144), (value - 0.75) * 4.0);
                    }

                    return vec4<f32>(color, 1.0);
                }
            `;

            try {
                const vertexModule = device.createShaderModule({ code: vertexShader });
                console.log('Vertex shader module created');

                const fragmentModule = device.createShaderModule({ code: fragmentShader });
                console.log('Fragment shader module created');

                const sampler = device.createSampler({
                    magFilter: 'linear',
                    minFilter: 'linear',
                });

                renderPipeline = device.createRenderPipeline({
                    layout: 'auto',
                    vertex: {
                        module: vertexModule,
                        entryPoint: 'main',
                    },
                    fragment: {
                        module: fragmentModule,
                        entryPoint: 'main',
                        targets: [{
                            format: navigator.gpu.getPreferredCanvasFormat(),
                        }],
                    },
                    primitive: {
                        topology: 'triangle-list',
                    },
                });
                console.log('Render pipeline created');
            } catch (error) {
                console.error('Render pipeline creation error:', error);
                throw error;
            }
        }

        async function initAudio() {
            try {
                micStream = await navigator.mediaDevices.getUserMedia({ audio: true });

                audioContext = new AudioContext({ sampleRate: 44100 });

                // Create AnalyserNode to get time-domain data
                analyserNode = audioContext.createAnalyser();
                analyserNode.fftSize = FFT_SIZE;

                // Create buffer for time-domain data
                audioDataArray = new Float32Array(FFT_SIZE);

                const source = audioContext.createMediaStreamSource(micStream);
                source.connect(analyserNode);

                console.log('Audio initialized successfully');
                console.log('Sample rate:', audioContext.sampleRate);
                console.log('Analyser FFT size:', analyserNode.fftSize);

                return true;
            } catch (error) {
                showError('Microphone access error: ' + error.message);
                return false;
            }
        }

        function processAudioData() {
            if (!device || !analyserNode) return;

            // Get time-domain data from analyser
            analyserNode.getFloatTimeDomainData(audioDataArray);

            // Store input data if recording
            if (isRecording) {
                recordedInputChunks.push(new Float32Array(audioDataArray));
            }

            // Debug: Check if we're getting audio data
            const audioLevel = Math.max(...audioDataArray.map(Math.abs));
            if (inputCurrentRow % 60 === 0) { // Log every 60 frames (~1 second)
                console.log('Audio level:', audioLevel.toFixed(4), 'Current row:', inputCurrentRow);
            }

            // Update debug display
            if (inputCurrentRow % 10 === 0) {
                const debugDiv = document.getElementById('debug');
                if (debugDiv) {
                    const status = isRecording ? 'RECORDING' : 'STOPPED';
                    debugDiv.textContent = `${status} | Audio Level: ${audioLevel.toFixed(4)} | Row: ${inputCurrentRow} | Chunks: ${recordedInputChunks.length}`;
                }
            }

            // Copy audio data to GPU buffer
            device.queue.writeBuffer(audioBuffer, 0, audioDataArray);

            // Update current row parameter
            device.queue.writeBuffer(inputParamsBuffer, 0, new Uint32Array([inputCurrentRow]));

            // Compute FFT and update input waterfall
            const commandEncoder = device.createCommandEncoder();

            // Run FFT
            const fftPass = commandEncoder.beginComputePass();
            fftPass.setPipeline(fftPipeline);
            const fftBindGroup = device.createBindGroup({
                layout: fftPipeline.getBindGroupLayout(0),
                entries: [
                    { binding: 0, resource: { buffer: audioBuffer } },
                    { binding: 1, resource: { buffer: fftBuffer } }
                ]
            });
            fftPass.setBindGroup(0, fftBindGroup);
            fftPass.dispatchWorkgroups(Math.ceil(FFT_SIZE / 64));
            fftPass.end();

            // Update input waterfall texture
            const scrollPass = commandEncoder.beginComputePass();
            scrollPass.setPipeline(scrollPipeline);
            const scrollBindGroup = device.createBindGroup({
                layout: scrollPipeline.getBindGroupLayout(0),
                entries: [
                    { binding: 0, resource: inputWaterfallTexture.createView() },
                    { binding: 1, resource: { buffer: fftBuffer } },
                    { binding: 2, resource: { buffer: inputParamsBuffer } }
                ]
            });
            scrollPass.setBindGroup(0, scrollBindGroup);
            scrollPass.dispatchWorkgroups(Math.ceil(WATERFALL_WIDTH / 64));
            scrollPass.end();

            device.queue.submit([commandEncoder.finish()]);

            // Increment row for next frame
            inputCurrentRow = (inputCurrentRow + 1) % WATERFALL_HEIGHT;
        }

        let renderCount = 0;

        function render() {
            if (!device || !inputContext || !outputContext) {
                console.error('Render called but device or contexts are null');
                return;
            }

            renderCount++;

            // Process audio data each frame
            processAudioData();

            try {
                const sampler = device.createSampler({
                    magFilter: 'linear',
                    minFilter: 'linear',
                });

                // Render input canvas
                const inputEncoder = device.createCommandEncoder();
                const inputTextureView = inputContext.getCurrentTexture().createView();

                const inputRenderPass = inputEncoder.beginRenderPass({
                    colorAttachments: [{
                        view: inputTextureView,
                        loadOp: 'clear',
                        clearValue: { r: 0, g: 0, b: 0, a: 1 },
                        storeOp: 'store',
                    }],
                });

                const inputBindGroup = device.createBindGroup({
                    layout: renderPipeline.getBindGroupLayout(0),
                    entries: [
                        { binding: 0, resource: inputWaterfallTexture.createView() },
                        { binding: 1, resource: sampler },
                        { binding: 2, resource: { buffer: inputParamsBuffer } }
                    ]
                });

                inputRenderPass.setPipeline(renderPipeline);
                inputRenderPass.setBindGroup(0, inputBindGroup);
                inputRenderPass.draw(6);
                inputRenderPass.end();

                device.queue.submit([inputEncoder.finish()]);

                // Render output canvas
                const outputEncoder = device.createCommandEncoder();
                const outputTextureView = outputContext.getCurrentTexture().createView();

                const outputRenderPass = outputEncoder.beginRenderPass({
                    colorAttachments: [{
                        view: outputTextureView,
                        loadOp: 'clear',
                        clearValue: { r: 0, g: 0, b: 0, a: 1 },
                        storeOp: 'store',
                    }],
                });

                const outputBindGroup = device.createBindGroup({
                    layout: renderPipeline.getBindGroupLayout(0),
                    entries: [
                        { binding: 0, resource: outputWaterfallTexture.createView() },
                        { binding: 1, resource: sampler },
                        { binding: 2, resource: { buffer: outputParamsBuffer } }
                    ]
                });

                outputRenderPass.setPipeline(renderPipeline);
                outputRenderPass.setBindGroup(0, outputBindGroup);
                outputRenderPass.draw(6);
                outputRenderPass.end();

                device.queue.submit([outputEncoder.finish()]);
            } catch (error) {
                console.error('Render error:', error);
                isRunning = false;
            }

            if (isRunning) {
                animationId = requestAnimationFrame(render);
            }
        }

        async function start() {
            showStatus('Initializing...');

            document.getElementById('startBtn').disabled = true;

            if (!await initWebGPU()) {
                document.getElementById('startBtn').disabled = false;
                return;
            }

            if (!await initAudio()) {
                document.getElementById('startBtn').disabled = false;
                return;
            }

            // Clear previous recordings
            recordedInputChunks = [];
            recordedOutputChunks = [];
            inputAudioBuffer = null;
            outputAudioBuffer = null;
            inputCurrentRow = 0;
            outputCurrentRow = 0;

            isRunning = true;
            isRecording = true;
            document.getElementById('stopBtn').disabled = false;
            document.getElementById('processBtn').disabled = true;
            document.getElementById('playInputBtn').disabled = true;
            document.getElementById('playOutputBtn').disabled = true;
            showStatus('Recording - microphone active', true);

            render();
        }

        function stop() {
            isRunning = false;
            isRecording = false;

            if (animationId) {
                cancelAnimationFrame(animationId);
            }

            if (analyserNode) {
                analyserNode.disconnect();
                analyserNode = null;
            }

            if (audioContext) {
                audioContext.close();
                audioContext = null;
            }

            if (micStream) {
                micStream.getTracks().forEach(track => track.stop());
                micStream = null;
            }

            document.getElementById('startBtn').disabled = false;
            document.getElementById('stopBtn').disabled = true;

            // Enable process button if we have recorded data
            if (recordedInputChunks.length > 0) {
                document.getElementById('processBtn').disabled = false;
                showStatus(`Recording stopped. Captured ${recordedInputChunks.length} chunks. Click "Process Audio" to continue.`);
            } else {
                showStatus('Stopped - no data recorded');
            }
        }

        // PLACEHOLDER AUDIO PROCESSING FUNCTION
        // Replace this with your own audio processing algorithm
        function processAudioBuffer(inputData) {
            // For now, just copy the input to output
            // You can replace this with any processing you want:
            // - Noise reduction
            // - Equalization
            // - Compression
            // - etc.
            console.log('Processing audio... (currently just copying input to output)');
            return new Float32Array(inputData);
        }

        async function processRecording() {
            if (recordedInputChunks.length === 0) {
                showError('No recorded data to process');
                return;
            }

            showStatus('Processing audio...');
            document.getElementById('processBtn').disabled = true;

            // Concatenate all input chunks into a single buffer
            const totalSamples = recordedInputChunks.reduce((sum, chunk) => sum + chunk.length, 0);
            const inputData = new Float32Array(totalSamples);
            let offset = 0;
            for (const chunk of recordedInputChunks) {
                inputData.set(chunk, offset);
                offset += chunk.length;
            }

            console.log(`Processing ${totalSamples} samples (${(totalSamples / SAMPLE_RATE).toFixed(2)} seconds)`);

            // Process the audio (placeholder - just copies for now)
            const outputData = processAudioBuffer(inputData);

            // Split output back into chunks for visualization
            recordedOutputChunks = [];
            for (let i = 0; i < outputData.length; i += FFT_SIZE) {
                const chunk = outputData.slice(i, i + FFT_SIZE);
                if (chunk.length === FFT_SIZE) {
                    recordedOutputChunks.push(chunk);
                }
            }

            // Visualize the output
            await visualizeOutput();

            // Enable playback buttons
            document.getElementById('playInputBtn').disabled = false;
            document.getElementById('playOutputBtn').disabled = false;
            showStatus('Processing complete. You can now play input/output.', true);
        }

        async function visualizeOutput() {
            if (!device || recordedOutputChunks.length === 0) return;

            showStatus('Generating output waterfall...');
            outputCurrentRow = 0;

            for (const chunk of recordedOutputChunks) {
                // Copy audio data to GPU buffer
                device.queue.writeBuffer(audioBuffer, 0, chunk);
                device.queue.writeBuffer(outputParamsBuffer, 0, new Uint32Array([outputCurrentRow]));

                // Compute FFT and update output waterfall
                const commandEncoder = device.createCommandEncoder();

                // Run FFT
                const fftPass = commandEncoder.beginComputePass();
                fftPass.setPipeline(fftPipeline);
                const fftBindGroup = device.createBindGroup({
                    layout: fftPipeline.getBindGroupLayout(0),
                    entries: [
                        { binding: 0, resource: { buffer: audioBuffer } },
                        { binding: 1, resource: { buffer: fftBuffer } }
                    ]
                });
                fftPass.setBindGroup(0, fftBindGroup);
                fftPass.dispatchWorkgroups(Math.ceil(FFT_SIZE / 64));
                fftPass.end();

                // Update output waterfall texture
                const scrollPass = commandEncoder.beginComputePass();
                scrollPass.setPipeline(scrollPipeline);
                const scrollBindGroup = device.createBindGroup({
                    layout: scrollPipeline.getBindGroupLayout(0),
                    entries: [
                        { binding: 0, resource: outputWaterfallTexture.createView() },
                        { binding: 1, resource: { buffer: fftBuffer } },
                        { binding: 2, resource: { buffer: outputParamsBuffer } }
                    ]
                });
                scrollPass.setBindGroup(0, scrollBindGroup);
                scrollPass.dispatchWorkgroups(Math.ceil(WATERFALL_WIDTH / 64));
                scrollPass.end();

                device.queue.submit([commandEncoder.finish()]);

                outputCurrentRow = (outputCurrentRow + 1) % WATERFALL_HEIGHT;
            }

            // Render both canvases one final time
            const sampler = device.createSampler({
                magFilter: 'linear',
                minFilter: 'linear',
            });

            // Render output canvas
            const outputEncoder = device.createCommandEncoder();
            const outputTextureView = outputContext.getCurrentTexture().createView();

            const outputRenderPass = outputEncoder.beginRenderPass({
                colorAttachments: [{
                    view: outputTextureView,
                    loadOp: 'clear',
                    clearValue: { r: 0, g: 0, b: 0, a: 1 },
                    storeOp: 'store',
                }],
            });

            const outputBindGroup = device.createBindGroup({
                layout: renderPipeline.getBindGroupLayout(0),
                entries: [
                    { binding: 0, resource: outputWaterfallTexture.createView() },
                    { binding: 1, resource: sampler },
                    { binding: 2, resource: { buffer: outputParamsBuffer } }
                ]
            });

            outputRenderPass.setPipeline(renderPipeline);
            outputRenderPass.setBindGroup(0, outputBindGroup);
            outputRenderPass.draw(6);
            outputRenderPass.end();

            device.queue.submit([outputEncoder.finish()]);
        }

        function playInput() {
            playAudioBuffer(recordedInputChunks, 'input');
        }

        function playOutput() {
            playAudioBuffer(recordedOutputChunks, 'output');
        }

        function playAudioBuffer(chunks, label) {
            if (chunks.length === 0) {
                showError(`No ${label} data to play`);
                return;
            }

            // Concatenate chunks
            const totalSamples = chunks.reduce((sum, chunk) => sum + chunk.length, 0);
            const audioData = new Float32Array(totalSamples);
            let offset = 0;
            for (const chunk of chunks) {
                audioData.set(chunk, offset);
                offset += chunk.length;
            }

            // Create audio context and buffer
            const playbackContext = new AudioContext({ sampleRate: SAMPLE_RATE });
            const buffer = playbackContext.createBuffer(1, audioData.length, SAMPLE_RATE);
            buffer.copyToChannel(audioData, 0);

            // Play
            const source = playbackContext.createBufferSource();
            source.buffer = buffer;
            source.connect(playbackContext.destination);
            source.start();

            const duration = audioData.length / SAMPLE_RATE;
            showStatus(`Playing ${label} (${duration.toFixed(2)}s)...`, true);

            source.onended = () => {
                playbackContext.close();
                showStatus(`${label} playback finished`);
            };
        }

        function showStatus(message, isSuccess = false) {
            const statusDiv = document.getElementById('status');
            statusDiv.textContent = message;
            statusDiv.style.color = isSuccess ? '#22c55e' : '#fff';
            statusDiv.className = 'status';
        }

        function showError(message) {
            const statusDiv = document.getElementById('status');
            statusDiv.innerHTML = `<div class="error">${message}</div>`;
        }
    </script>
</body>
</html>
